---
title: "Final Paper"
author: "STOR 320.02 Group 2"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
library(xtable)
library(ggplot2)
library(readr)
library(kableExtra)
library(knitr)
library(modelr)
library(broom)
library(purrr)
library(caret)
library(leaps)
library(plotly)
library(tmap)
library(ggspatial)
library(sf)
library(readr)
library(dplyr)
library(htmlTable)

SALARY <- read_csv("SalaryDataFinal.csv") %>%
mutate(NumSkills=Python+spark+aws+excel+sql+sas+keras+pytorch+scikit+tensor+hadoop+tableau+bi+flink+mongo+google_an)

  
cities <- st_read("stanford-bx729wr3020-shapefile/bx729wr3020.shp")
coord <- read.csv("simplemaps_uscities_basicv1.74/uscities.csv")

ModelData = read.csv("ModelDATA.csv") %>% select(-X) %>% mutate(AdjustedSalary = sqrt(AdjustedSalary))
#Put Necessary Libraries Here
```

# INTRODUCTION

> According to the US Bureau of Labor Statistics, demand for jobs in data science is at an all time high. In the next decade, the number of data science jobs is projected to increase by nearly 12 million. Data science is more in demand than ever, and for good reason; data scientists work at the intersections of mathematics, technology, business, and countless other fields to provide insights and analysis. At the same time, the field of data science is continuously evolving, and the requirements of new data science jobs are changing constantly. 

> With this in mind, we set out to explore what optimal data science job applicants look like. First, we wanted to look at what factors are most important for determining an applicant’s future salary, whether these factors were related to type of company, an applicant’s skills and prior knowledge, or even the job description itself. A profession’s salary is on every future employee’s mind. Everyone that has, or will ever, hold a job wants to make as much money as they can. Therefore, modeling which variables are most important to a high-paying job can help future employees determine which jobs they should apply to.

> For our second question, we looked more specifically at which skills a person should learn in order to maximize their average salary as well as the amount of jobs that are available to individuals with certain skills. Knowing which skills are most important for particular jobs is beneficial to anyone as it allows them to maximize the benefits they can receive for the effort that it requires to become proficient in a skill. The goal from this exploration will be to allow each person to get a glimpse into what type of specialization will allow them to maximize their salary potential as well as ability to get a job that they find rewarding. 


# DATA

```{r, echo=F}
FullData = read.csv("SalaryDataFinal.csv")
FullData= FullData %>% select(-X) %>% mutate(NumSkills=Python+spark+aws+excel+sql+sas+keras+pytorch+scikit+tensor+hadoop+tableau+bi+flink+mongo+google_an) %>% 
  mutate(seniority_by_title = ifelse(seniority_by_title=="jr", "na", seniority_by_title)) %>%
  mutate(Size = ifelse(Company.Name=="Kronos Bio", "51 - 200", Size))

FullData$Size = factor(FullData$Size, levels = c("1 - 50", "51 - 200", "201 - 500", "501 - 1000", "1001 - 5000", "5001 - 10000", "10000+"))
FullData$Revenue = factor(FullData$Revenue, levels = c("Unknown / Non-Applicable", "Less than $1 million (USD)", "$1 to $5 million (USD)", "$5 to $10 million (USD)", "$10 to $25 million (USD)", "$25 to $50 million (USD)", "$50 to $100 million (USD)", "$100 to $500 million (USD)", "$500 million to $1 billion (USD)", "$1 to $2 billion (USD)", "$2 to $5 billion (USD)", "$5 to $10 billion (USD)", "$10+ billion (USD)"))
FullData$seniority_by_title = factor(FullData$seniority_by_title, levels = c("na","sr"))
FullData$Degree = factor(FullData$Degree, levels = c("na","M","P"))
FullData = FullData %>% select(-c(index,Rating, Company.Name, Location, Headquarters, Founded, Industry, Competitors, Lower.Salary, Upper.Salary, company_txt, Job.Location)) %>%
  mutate(Type.of.ownership = ifelse(Type.of.ownership=="Company - Private" | Type.of.ownership=="Company - Public", Type.of.ownership, "Other")) %>%
  na.omit(ModelData) %>%
  mutate(Revenue = ifelse(Revenue=="$1 to $5 million (USD)" | 
                                                    Revenue=="$5 to $10 million (USD)" | 
                                                    Revenue=="$10 to $25 million (USD)" | 
                                                    Revenue=="$25 to $50 million (USD)", "Small", ifelse(Revenue=="$50 to $100 million (USD)" |
                                                                                                           Revenue=="$100 to $500 million (USD)" |
                                                                                                           Revenue=="$500 million to $1 billion (USD)", "Medium", 
                                                                                                         ifelse(Revenue == "Unknown / Non-Applicable", "Unknown / Non-Applicable", "Large")))) %>%
  mutate(Sector = ifelse(Sector=="Non-Profit" |
                                                    Sector=="Transportation & Logistics" | 
                                                    Sector=="Travel & Tourism" | 
                                                    Sector=="Non-Profit" |
                                                      Sector=="Media" | 
                                                      Sector=="Telecommunications" | 
                                                      Sector=="Arts, Entertainment & Recreation" | 
                                                      Sector=="Construction, Repair & Maintenance" | 
                                                      Sector=="Consumer Services" | 
                                                      Sector=="Mining & Metals" |
                                                      Sector=="Agriculture & Forestry"|
                                                      Sector=="Government" |
                                                      Sector=="Oil, Gas, Energy & Utilities", "Other", 
                                                      ifelse(Sector=="Insurance"|
                                                      Sector=="Finance"|
                                                      Sector=="Real Estate", "Finance, Insurance & Real Estate", Sector)))
```


> The data that we used was published on Kaggle.com on December 29, 2021 by a user named Nikhil Bhathi. The dataset was scrapped from Glassdoor.com, a job-searching website that also allows employees to review companies. According to Bhathi, he scraped job postings related to the position of "Data Scientist" in the USA. The dataset has 742 observations, with each observation being one job posting. The following figure shows the geographical distribution of the jobs in the dataset:

```{r, echo=F}
coord <- unite(coord, "Location", city_ascii:state_id, sep=", ", remove=FALSE)
cities <- unite(cities, "Location", name, state, sep=", ", remove=FALSE)

salary2 <- merge(SALARY, coord, by="Location")
cities2 <- merge(cities, salary2, by="Location")
```


```{r, echo=F, warning=FALSE, message=FALSE}

tmap_mode("view")
tm_shape(cities2) +
  tm_dots("Location", size = 0.001,
  shape = 19,
  title = NA,
  legend.show = FALSE, col="black")
```



> The dataset originally contained 42 columns. This number was then reduced to 27 after dropping the columns that we were not interested in. Finally, we added 3 more variables, either derived from within the data or merged from outside data, making the final data consist of 30 columns. The following table shows 14 variables in our dataset: 

```{r, echo=F}
head(FullData %>% select(-c(10:25)), 3) %>%
  kbl("html", escape=F) %>%
  kable_material(c("hover"), fixed_thead = T) %>%
  scroll_box(width="100%")
```

* _Job.Title_ -- The title of the job position, including: Data Scientist, Other Scientist, Data Analyst, Data Engineer, and Machine Learning Engineer.
* _Size_ -- Range of number of employees in the company, including: 1 -- 50, 51 -- 200, 201 -- 500, 501 -- 1000, 1001 -- 5000, 5001 -- 10000, and 10000+. 
* _Type.of.ownership_ -- A company is either Private, Public, or Other. 
* _Sector_ -- Sector of the company, 10 total, including: Information Technology, Business Services, Education, etc. We combined certain sectors according to the Standard Industrial Classification and sectors with low frequencies into Other. 
* _Revenue_ -- Revenue of the company per year. The original dataset had number ranges, but after researching on the various of way companies are classified, we combined them into three levels: Small (1 to 50 million), Medium (50 million to 1 billion), and Large (over 1 billion). 
* _Hourly_ -- Binary value showing whether the salary was paid hourly, 1 being yes, 0 being no. 
* _Employer.provided_ -- Binary value showing whether the salary was provided by the employee of the company, 1 being yes, 0 being no. 
* _AvgSalary_ -- The salary of the job, in thousands of dollars. 
* _Age_ -- Age of the company
* _seniority_by_title_ -- Seniority of the position, including na (none) and sr (senior).
* _Degree_ -- Whether the job gives experience credit for master (M) or Ph.D (P) degree, or none (na).
* _COL_ -- Cost of Living index of the job location. This column was merged in from a outside dataset from advisorsmith.com. The cost of living index is a number relative to 100, the average cost of living in the US. For example, a COL of 120 means a cost of living that is 20% higher than the national average. 
* _AdjustedSalary_ -- Since the actual salary is influenced heavily by the cost of living of the job location, we wanted a salary variable that is not biased job location. This variable is derived from AvgSalary / COL. This variable will be used when we refer to salary in this project.
* _NumSkills_ -- Number of skills required for the job position. Min: 0. Max: 10.

> The other 16 columns are for 16 different skills: Python, Spark, AWS, Excel, SQL, SAS, Keras, Pytorch, Scikit, TensorFlow, Hadoop, tableau, PowerBi, Flink, MongoDB, and Google Analytics. 1 means the corresponding skill is required, while 0 means the opposite. 


# RESULTS

## Question 1 

```{r, echo=FALSE}
Model0 = lm(AdjustedSalary ~ 1, data=ModelData)
Model1 = lm(AdjustedSalary ~ Job.Title, data=ModelData)
Model2 = lm(AdjustedSalary ~ Job.Title + seniority_by_title, data=ModelData)
Model3 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly, data=ModelData)
Model4 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue, data=ModelData)
Model5 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership, data=ModelData)
Model6 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age, data=ModelData)
Model7 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age + Employer.provided, data=ModelData)
Model8 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills, data=ModelData)
Model9 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills + Size, data=ModelData)
Model10 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills + Size + Degree, data=ModelData)
ModelF = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills + Size + Degree + Sector, data=ModelData)

Model11 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + NumSkills + Degree + Job.Title:seniority_by_title + NumSkills:Degree, data=ModelData)
Model11b = lm(AdjustedSalary ~ (Job.Title + seniority_by_title + NumSkills + Degree)^2, data=ModelData)
Model12 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Size + Size:Type.of.ownership, data=ModelData)
Model13 = lm(AdjustedSalary ~ Hourly + Revenue + Type.of.ownership + Age + Employer.provided + Size + Sector + Revenue:Hourly + Size:Type.of.ownership + Size:Sector + Sector:Age, data=ModelData)
Model14 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + Size + Sector + Job.Title:seniority_by_title + Size:Type.of.ownership, data=ModelData)
Model15 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Type.of.ownership + NumSkills + Size + Degree + Job.Title:seniority_by_title + Degree:NumSkills + Size:Type.of.ownership, data=ModelData)
Model16 = lm(AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills + Size + Degree + Sector + Job.Title:seniority_by_title + Degree:NumSkills + Size:Type.of.ownership + Revenue:Hourly + Size:Sector + Sector:Age, data=ModelData)
```

```{r, echo=FALSE}
ReturnModelfun = function(data, mod){
  return(mod)
}
```

```{r, warning=F, echo=FALSE}
set.seed(123)
ModelData2 = ModelData %>% crossv_kfold(10)
ModelError = matrix(NA, nrow=19, ncol=2)
Models = list(Model0, Model1, Model2, Model3, Model4, Model5, Model6, Model7, Model8, Model9, Model10, ModelF, Model11, Model11b, Model12, Model13, Model14, Model15, Model16)
rownames(ModelError) <- c("Model0", "Model1", "Model2", "Model3", "Model4", "Model5", "Model6", "Model7", "Model8", "Model9", "Model10", "ModelF", "Model11", "Model11b", "Model12", "Model13", "Model14", "Model15", "Model16")

for(i in 1:19){
  ModelData3 = ModelData2 %>% 
      mutate(tr.model=map(train, ReturnModelfun, mod=Models[[i]]))
  ModelData4 = ModelData3 %>%
      mutate(predict=map2(test,tr.model,~augment(.y,newdata=.x))) %>%
      select(predict) %>%
      unnest()
  RMSE = sqrt(mean((ModelData4$.resid)^2))
  MAE = mean(abs(ModelData4$.resid))
  ModelError[i,1]=RMSE
  ModelError[i,2]=MAE
}
```

> To answer our first question, we wanted to fit a linear regression model to our data to predict the adjusted salaries of the jobs. The potential predictor variables include `Job.Title`, `Size`, `Type.of.ownership`, `Sector`, `Revenue`, `Hourly`, `Employer.provided`, `Age`, `seniority_by_title`, `Degree`, and `NumSkills`. We decided to take a square root transformation of the response variable, `AdjustedSalary`, to address the rightward skewness and make the values more normal. After extensive searching and exploring, we picked 19 competing models. The models were as follows: 

* Model0: _AdjustedSalary ~ 1_
* Model1: _AdjustedSalary ~ Job.Title_
* Model2: _AdjustedSalary ~ Job.Title + senioritybytitle_
* Model3: _AdjustedSalary ~ Job.Title + senioritybytitle + Hourly_
* Model4: _AdjustedSalary ~ Job.Title + senioritybytitle + Hourly + Revenue_
* Model5: _AdjustedSalary ~ Job.Title + senioritybytitle + Hourly + Revenue + Type.of.ownership_
* Model6: _AdjustedSalary ~ Job.Title + senioritybytitle + Hourly + Revenue + Type.of.ownership + Age_
* Model7: _AdjustedSalary ~ Job.Title + senioritybytitle + Hourly + Revenue + Type.of.ownership + Age + Employer.provided_
* Model8: _AdjustedSalary ~ Job.Title + senioritybytitle + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills_
* Model9: _AdjustedSalary ~ Job.Title + senioritybytitle + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills + Size_
* Model10: _AdjustedSalary ~ Job.Title + senioritybytitle + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills + Size + Degree_
* ModelF: _AdjustedSalary ~ Job.Title + senioritybytitle + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills + Size + Degree + Sector_
* Model11: _AdjustedSalary ~ Job.Title + senioritybytitle + NumSkills + Degree + Job.Title:senioritybytitle + NumSkills:Degree_
* Model11b: _AdjustedSalary ~ (Job.Title + seniority_by_title + NumSkills + Degree)^2_
* Model12: _AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Size + Size:Type.of.ownership_
* Model13: _AdjustedSalary ~ Hourly + Revenue + Type.of.ownership + Age + Employer.provided + Size + Sector + Revenue:Hourly + Size:Type.of.ownership + Size:Sector + Sector:Age_
* Model14: _AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + Size + Sector + Job.Title:seniority_by_title + Size:Type.of.ownership_
* Model15: _AdjustedSalary ~ Job.Title + seniority_by_title + Type.of.ownership + NumSkills + Size + Degree + Job.Title:seniority_by_title + Degree:NumSkills + Size:Type.of.ownership_
* Model16: _AdjustedSalary ~ Job.Title + seniority_by_title + Hourly + Revenue + Type.of.ownership + Age + Employer.provided + NumSkills + Size + Degree + Sector + Job.Title:seniority_by_title + Degree:NumSkills + Size:Type.of.ownership + Revenue:Hourly + Size:Sector + Sector:Age_

> Model 0 to 10 were the best models for each number of predictors according to the bi-directional stepwise procedure. It used AIC as the criteria, and each of the first 10 models had the lowest AIC value compared to the other models that had the same number of predictors. Since the majority of our predictors were categorical variables, we could not ignore the interactions between them. Different combinations of the predictors could potentially have different influences on the salary of the jobs. We found three interactions that were logical to include in our models: 

* Job.Title * seniority_by_title
* Degree * NumSkills
* Size * Type.of.ownership

> These interactions give further information about the position, the knowledge required, and the company of the job, respectively. Then, we used the FSA function from the rFSA package to further explore the interactions between the variables. The FSA is a function that is used for subset selection and the identification of interaction terms. It supports several model criteria, but we used the AIC and r-squared values in this project. Using the FSA function, we found three more interaction terms that were not yet identified: 

* Revenue * Hourly
* Size * Sector
* Sector * Age

> After finding six different interaction terms, we found 7 more models. Models 11 to 16 were different models that contained interaction terms. We chose models that had different combinations of predictors. Some models only had variables about the company, while some others only had variables about the job position and the knowledge required. 

>After having all of the competing models ready, we used the k-fold cross-validation method to test our models. We decided to use this cross-validation method because we had a relatively small dataset of around 700 observations, and we wanted to get the most out of it. We used a k-value of 10, and here is the result of all of the 19 models: 

```{r, echo=FALSE, warning=F}
ModelError.df = data.frame(Model = row.names(ModelError), ModelError) %>% rename(RMSE = X1, MAE = X2) %>% gather(key="Error", value="Value", RMSE:MAE)
ggplotly(ggplot(data=ModelError.df)+
  geom_bar(aes(x=fct_inorder(Model), y=Value, fill=Error), stat="identity", position="dodge", color="black", size=0.4, show.legend=F) +
  xlab("Model") +
  ggtitle("MAE and RMSE of All Models") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x=element_text(angle = 45, hjust=1)) +
  theme(axis.text.x=element_text(face=ifelse(ModelError.df$Model=="Model12"|ModelError.df$Model=="Model14"|ModelError.df$Model=="Model16","bold","plain")))+
  scale_fill_brewer(palette = "Purples"),tooltip = c("y","fill"))
```

> The MAE and the RMSE were used to compare the models. Model 16 was the best as it had the lowest MAE and RMSE values, with Models 14 and 12 in second and third place. It was no surprise that Model 16 was the best since it had all the predictors and interaction terms. What is interesting is that the second and third best models did not have any terms about education or the number of skills required. Information about the company and job position is far more important when determining the salary of a job. However, this could be a result of not including the variables that indicate the specific skills that the jobs require. We decided not to include them in the model because we will explore the specific skills more in-depth in the second part of the project. 

>Looking at Model 16 and its significant coefficients, we could learn many factors that impact the salary of a job positively and negatively. Machine learning engineers get paid the most, followed by data scientists and data engineers. Senior positions are, as expected, higher-paid. Generally, companies with large revenue pay more, but if a job from this kind of company is paid hourly, its salary will greatly decrease. Interestingly, hourly-paid jobs from companies with low or medium revenue do not have the same kind of drastic difference. Companies with 51–200 and 5001–10,000 employees pay the most, and private companies also pay more than public companies, regardless of size. Positions from the government, non-profit and universities pay more than those from public and private companies. Jobs that give experience credit for Ph.D degree pays more. A good rule of thumb when trying to find a company that pays well is to always search big. Big companies, both employee and revenue wise, generally pays more than medium size companies, regardless of the sector of the company. This difference is especially huge in the finance, insurance, real estate, health care and IT field. One shoud also avoid super small companies with less than 50 employees. 51-200 employees is the ideal range if one dislikes big companies. 

> Lastly, we want to further learn about our model by looking at the actual vs. predicted plot:

```{r, echo=FALSE, warning=F}
ModelData = add_predictions(data=ModelData, model = Model16, var = "Prediction") %>% mutate(AdjustedSalary=AdjustedSalary^2, Prediction=Prediction^2)
ggplotly(ggplot(data=ModelData)+
  geom_point(aes(x=AdjustedSalary, y=Prediction), shape=1, color = "steelblue") +
  geom_abline(slope=1, intercept = 0, color="red", alpha = 0.7)+
  theme_minimal())
```

> From the plot, we see a moderate fit. The model does a fair job of predicting salaries in the mid-range, around 1,000. However, we do see that our data tends to overpredict when the true salary is low and underpredict when the true salary is high. Again, the possible explanation is probably that we did not use any information about what skills are required for each job. We did not include potentially 16 more variables in our models because it would be way too complicated. Plus, the specific skills are a topic on their own that we will explore next. 


## Question 2

> For our second question, we decided to analyze which skills were optimal for future employees to learn. We also decided to look at the number of skills employees should learn to best serve them in the future. First, we examined the average salary of learning no skills, and compared it to the average salary of jobs that required each major individual skill, which includes Python, Excel, and SAS.

```{r, echo=FALSE}
SALARY %>%
  group_by(Python) %>%
  summarise(meanSalary = mean(AdjustedSalary)) %>%
  ungroup() %>%
  htmlTable()


SALARY %>%
  group_by(excel) %>%
  summarise(meanSalary = mean(AdjustedSalary)) %>%
  ungroup() %>%
  htmlTable()

SALARY %>%
  group_by(sas) %>%
  summarise(meanSalary = mean(AdjustedSalary)) %>%
  ungroup() %>%
  htmlTable()

```

> Here, we can see that the average salary of a job with no required skills is much lower than jobs that require skills like Python or SAS. The salary listed at the value “0 Python” for example, represents the mean salary of jobs that do not require Python. “1 Python” represents the mean salary of jobs that do require Python. Surprisingly, the mean salary of jobs that require Excel is lower than the mean salary of jobs that do not require Excel–perhaps this is a signal that jobs that require Excel are less technical than other jobs, and therefore, have lower salaries. 

>Next, we wanted to look at which skills were most in demand. To do this, we used a bar chart to examine which skills had more job listings that required the certain skill than did not.

```{r, echo=FALSE, message= FALSE, warning=FALSE}
library(gridExtra)
Skills_subset=c("Python", "excel", "sql")

plot1 = SALARY %>%
    group_by_at("Python") %>%
    summarise(n = n()) %>%
    ggplot() +
      geom_bar(aes_string(x="Python", y="n", fill="Python"), stat = "identity")+
      theme_classic()+
      coord_cartesian(ylim = c(0,800)) +
      ylab("Number of Jobs") +
      scale_x_continuous(breaks=0:1, labels=c("0","1"))+
  theme(legend.position = "none")
plot2 = SALARY %>%
    group_by_at("excel") %>%
    summarise(n = n()) %>%
    ggplot() +
      geom_bar(aes_string(x="excel", y="n", fill="excel"), stat = "identity")+
      theme_classic()+
      coord_cartesian(ylim = c(0,800)) +
      ylab("Number of Jobs") +
      scale_x_continuous(breaks=0:1, labels=c("0","1"))+
      theme(legend.position = "none")
plot3 = SALARY %>%
    group_by_at("sql") %>%
    summarise(n = n()) %>%
    ggplot() +
      geom_bar(aes_string(x="sql", y="n", fill="sql"), stat = "identity")+
      theme_classic()+
      coord_cartesian(ylim = c(0,800)) +
      ylab("Number of Jobs") +
      scale_x_continuous(breaks=0:1, labels=c("0","1"))+
  theme(legend.position = "none")
# for(i in Skills_subset){
#   plot = SALARY %>%
#     group_by_at(i) %>%
#     summarise(n = n()) %>%
#     ggplot() +
#       geom_bar(aes_string(x=i, y="n", fill=i), stat = "identity")+
#       theme_classic()+
#       coord_cartesian(ylim = c(0,800)) +
#       ylab("Number of Jobs") +
#       scale_x_continuous(breaks=0:1, labels=c("Does not require","Requires"))
# 
#   p=append(p,plot)
#   
# }
grid.arrange(plot1, plot2, plot3, ncol=3)
```

> The bar on the right side represents the amount of jobs that required a certain skill, while the left side represented the number of jobs that did not require the skill. These charts tell us that the highest-demand skills are Python, Excel, and SQL, as these skills were sought out by a majority of job listings. 

>From here, we wanted to see how many skills an applicant should learn to maximize their salary.

```{r, echo=FALSE, warning=FALSE}
SALARY%>%
  group_by(NumSkills) %>%
  summarise(meanSalary = mean(AdjustedSalary), sd = sd(AdjustedSalary), n=n()) %>%
  ggplot()+
  geom_bar(aes(x=NumSkills, y=meanSalary, fill=factor(ifelse(NumSkills==2, "Highlighted", "Normal"))), stat = "identity", show.legend = F, col="black")+
  geom_text(aes(x=NumSkills, y=500, label=n), col="white")+
  geom_pointrange(aes(x=NumSkills, y=meanSalary, ymin = meanSalary-1.96*sd/sqrt(n), ymax = meanSalary+1.96*sd/sqrt(n)), stat = "identity", show.legend = F, col="black", size=0.5, shape=16) +
  scale_x_continuous(breaks=seq(0, 10, by=1)) +
  scale_fill_brewer(palette = "Accent")+
  theme_minimal()+
  xlab("Number of Skills")+
  ylab("Average Adjusted Salary")

```

>  We used this graph to plot the number of skills and mean salary. It plays out like one would expect it to–learning no skills has the lowest mean salary. However, learning two skills seemed to have the highest increase, and seemed the most reasonable. Furthermore, the confidence intervals do not overlap, proving that learning 2 skills is the most efficient way to boost one's salary. 


>After looking at each combination of two-skill jobs, we found that learning Python and SAS together produces the highest expected salaries; however, we do not believe that this is the best pair of skills. We wanted to find a pair of skills that has the greatest salary, but also wanted to ensure that there was high availability for this job. In order to discover which jobs were high paying but also most available, we multiplied the annual salary by the amount of jobs that need this skill.

```{r, echo=FALSE, message=FALSE}
library(kableExtra)
Skills=c("Python","spark","aws","excel","sql","sas","keras","pytorch","scikit","tensor","hadoop","tableau","bi","flink","mongo","google_an")
twoSkillDATA = SALARY %>%
  filter(NumSkills==2) %>%
  group_by_at(Skills) %>%
  summarise(n=n(), meanSalary=mean(AdjustedSalary), sd=sd(AdjustedSalary)) %>%
  mutate(TotalMoney = n*meanSalary) %>%
  mutate(lb=n*(meanSalary-1.96*sd/sqrt(n)), ub=n*(meanSalary+1.96*sd/sqrt(n))) %>%
  arrange(desc(TotalMoney)) %>%
  ungroup()

twoSkillMatrix = matrix(0, nrow=16, ncol=16)
temp = twoSkillDATA[1:16]
temp2 = t(apply(temp, 1, function(x)which(x>0)))
for(i in 1:18){
  row = temp2[i,1]
  col = temp2[i,2]
  v=round(twoSkillDATA$TotalMoney[i],0)
  twoSkillMatrix[row,col]=v
}

oneSkillDATA = SALARY %>%
  filter(NumSkills==1) %>%
  group_by_at(Skills) %>%
  summarise(n=n(), meanSalary=mean(AdjustedSalary), sd=sd(AdjustedSalary)) %>%
  mutate(TotalMoney = n*meanSalary) %>%
  mutate(lb=n*(meanSalary-1.96*sd/sqrt(n)), ub=n*(meanSalary+1.96*sd/sqrt(n))) %>%
  arrange(desc(TotalMoney)) %>%
  ungroup()
temp3 = oneSkillDATA[1:16]
temp4 = (apply(temp3, 1, function(x)which(x>0)))
for(i in 1:6){
  row = temp4[i]
  col = temp4[i]
  v=round(oneSkillDATA$TotalMoney[i],0)
  twoSkillMatrix[row,col]=v
}
makeSymm <- function(m) {
   m[lower.tri(m)] <- t(m)[lower.tri(m)]
   return(m)
}
twoSkillMatrix = makeSymm(twoSkillMatrix)
rownames(twoSkillMatrix)=Skills
colnames(twoSkillMatrix)=Skills
twoSkillMatrix2 <- data.frame(Skills = row.names(twoSkillMatrix), twoSkillMatrix)
for (i in 1:16) {
  twoSkillMatrix2[i,i+1] <- cell_spec(twoSkillMatrix2[i, i+1], "html", bold=F, background = "lightgrey")
}

twoSkillMatrix2 %>%
  select(-Skills) %>%
  kbl("html", escape=F) %>%
  kable_material(c("hover"), fixed_thead = T)%>%
  scroll_box(width="100%")
```

>  This matrix depicts the total money invested in these jobs: a larger number means that they are highly sought after and highly paid. After this method, we discovered that the best two skills one should learn to easily obtain a high-salary position were Python and Excel. 

> Continuing our exploration we wanted to help out someone who already knows one language and is deciding which language they should learn next. We created a correlation matrix to show how many jobs require a specific additional skill, given you already know a specific skill. 


```{r,echo=FALSE, warning=FALSE}
SkillData= SALARY %>% select(c(Python:google_an))
Skills=c("Python","spark","aws","excel","sql","sas","keras","pytorch","scikit","tensor","hadoop","tableau","bi","flink","mongo","google_an")
SkillCorMatrix = matrix(NA, nrow=16, ncol=16)
rownames(SkillCorMatrix)=Skills
colnames(SkillCorMatrix)=Skills
for(i in 1:16){
  t = SkillData %>%
    filter(eval(as.name(Skills[i]))==1)
  total = nrow(t)
  for(j in 1:16){
    t2 = t %>%
      filter(eval(as.name(Skills[j]))==1)
    subtotal = nrow(t2)
    r = subtotal/total
    SkillCorMatrix[i,j] = round(r, 2) 
  }
                
}

SkillCorMatrix2 <- data.frame(Skill1 = row.names(SkillCorMatrix), SkillCorMatrix)
SkillCorMatrix2 = pivot_longer(data = SkillCorMatrix2, 
                          cols = c(2:17),
                          names_to = "Skill2", 
                          values_to = "Prop")

ggplot(data=SkillCorMatrix2) +
  geom_tile(aes(x=fct_inorder(Skill2), y=fct_inorder(Skill1), fill=Prop), color="black", lwd=0.5)+
  scale_y_discrete(limits=rev) +
  geom_text(aes(x=fct_inorder(Skill2), y=fct_inorder(Skill1), fill=Prop, label=Prop))+
  theme(axis.text.x=element_text(angle = 45, hjust=1)) +
  scale_fill_gradient(low="white", high="red") +
  xlab("Skill2")+
  ylab("Skill1")+
  theme(legend.position = "none")


```

> This correlation matrix describes all job postings, not just those that require two skills   For example, as seen in the matrix below the intersection between Spark and Python is 0.85. This number represents that out of all jobs that require Spark 85% of them also require Python. If we look at the intersection between Spark and SAS the correlation matrix shows 0.06. This means that out of all the skills that require Spark, only 6% also require SAS. This is very useful information as someone who already knows spark will have a better direction to which skill they should learn next. As the correlation between Spark and Python is very high, it represents that a job seeker that already knows Spark should focus their efforts on learning Python rather than SAS, as it would increase the amount of available jobs more. 



# CONCLUSION

>The goal of the first question was to predict the salary of a job given its relevant information about the position, the company, and the required knowledge. Our best models were Model 16, 14, and 12. We learned that without knowing what specific skills are required for a job, the number of skills required does not impact the salary significantly. Information about the company and the specific job title were more important predictors. Interactions between the variables also provided a deeper relationship between the predictors and the salary. While the single terms generally hold, the interaction terms give us a closer look at the influences of the variables. This result is valuable and relevant today because it gives job seekers ideas and information about what to look for in a job and the company that is offering it. With this information, one knows whether the characteristics of a company could potentially have a positive or negative influence on the salary. This kind of information is crucial for job seekers, especially inexperienced students right out of college, as it saves them energy and time, which are two of the most valuable things for people looking for jobs. This information can also give employers insight into the current or future job position of the company. Using the model, employers can get a sense of the average salary of a particular job type at a company on the market. Employers can adjust the salary according to the "expected" salary at their company. 

>The goal of question two was to narrow down which skills were best to learn to maximize job salary. Here, we found that learning two skills together proved to have the highest salary outcomes, and among the two skill combinations, Python and SAS were the best to learn together to maximize salary. Although Python and Excel have a lower estimated salary, there is more total money invested in this position, proving it to be a highly available and well-paying job. However, if a person were to learn a single skill, they were best learning Python, Excel, or SQL. These results can serve as a guide for future applicants looking to maximize their salary by learning one skill. 

>To improve our model, we could gather more job postings to increase the accuracy of our predictions. Another way to improve our model is to broaden our investigation into more than two skills. Right now, our questions help job-seekers figure out which jobs are available, and which two skills they should learn to get paid the most. But now the question is: what happens after someone learns two skills? One way our investigation could be continued is by creating a model that would allow you to select the first two skills you already know. It could then provide a matrix for which skill you should learn next to maximize your salary, or to increase your job availability. This would be important for graduate students, or early employees looking to switch jobs. 

>Another problem with our model is that acquiring a job is not only about the “hard-skills”. The interviewing process is long and requires a lot of interpersonal and communication skills.  Our model does not account for any of these soft skills, or past experiences such as leadership positions and community involvement. A model that depicts a more holistic depiction of what employers are looking for such as “strong leadership skills” or “self-starter” may prove useful to help students gain an upper hand in these interviews. One way we could do this is by making all of the required bullets different arrays and then analyzing these arrays to see which soft-skills most employers are looking for. 
 








